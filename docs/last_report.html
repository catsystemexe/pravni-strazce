<!DOCTYPE html>
<html lang="cs">
<head>
    <meta charset="UTF-8">
    <title>Právní strážce – poslední report agenta</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
            margin: 2rem;
            line-height: 1.5;
            background: #0f172a;
            color: #e5e7eb;
        }
        pre {
            white-space: pre-wrap;
            word-wrap: break-word;
            background: #020617;
            padding: 1.5rem;
            border-radius: 0.75rem;
            border: 1px solid #1f2937;
            box-shadow: 0 10px 30px rgba(0,0,0,0.5);
        }
        h1 {
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }
        .meta {
            font-size: 0.85rem;
            color: #9ca3af;
            margin-bottom: 0.5rem;
        }
    </style>
</head>
<body>
    <h1>Právní strážce – poslední report agenta</h1>
    <div class="meta">Tento soubor generuje GitHub Action z repozitáře catsystemexe/pravni-strazce.</div>
    <pre>================================================================================
ODPOVĚĎ CODING AGENTA:

## 1. Co chápu ze struktury projektu

- Projekt je core framework pro „Právní strážce“, rozdělený na:
  - `runtime/` (orchestrátor, config loader),
  - `engines/` (core_legal, judikatura, risk, intent, output, memory, safety),
  - `llm/` (LLM klient + konfigurace + prompts),
  - `data/` (intents, doménové schéma),
  - `templates/` (výstupní šablony),
  - `packs/` (product/packs konfigurace),
  - API vrstvy (`api/cli.py`, `api/http_server.py`),
  - nástroje (`tools/validate_data.py`, `tools/apply_policy.py`).
- LLM integrace je v přechodovém stavu:
  - `llm/client.py` = jednoduchý klient s env řízeným backendem (`mock` / `openai`), bez čtení `llm/config.yaml`, s voláním OpenAI přes `OPENAI_MODEL` env.
  - `llm/config.py` = jiný LLM klient, který:
    - načítá `llm/config.yaml` přes `runtime.config_loader.load_yaml`,
    - má vlastní `LLMMessage` a `LLMClient`,
    - vybírá model/temperature/max_tokens podle `use_case` (legal_analysis vs helper),
    - předpokládá přítomný `OPENAI_API_KEY` a nepodporuje fallback na mock.
  - `llm/config.yaml` = centrální YAML s modely a parametry.

Dnes tedy existují dva paralelní koncepty LLM klienta a dva různé zdroje konfigurace modelu (env vs YAML).

---

## 2. Stručný report a návrh směru

### 2.1 Co jsem našel v `llm/`

1. `llm/client.py`
   - Definuje:
     - `Role` = Literal[&quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;].
     - `LLMMessage` (role, content).
     - `LLMClient` s:
       - autodetekcí backendu z `LLM_BACKEND` (`mock` default).
       - lazy importem `openai.OpenAI`, fallback na mock pokud chybí `OPENAI_API_KEY` nebo import selže.
       - metodou `chat(use_case, messages, temperature=None, max_tokens=None)`:
         - pokud backend == &quot;openai&quot; → `_chat_openai(...)`, jinak `_chat_mock(...)`.
       - `_chat_openai`:
         - bere model z `OPENAI_MODEL` (default `gpt-4.1-mini`),
         - volá `client.chat.completions.create`.
       - `_chat_mock`:
         - rozlišuje `use_case` aspoň pro:
           - `&quot;legal_analysis&quot;` → textový mock,
           - `&quot;jurisprudence_search&quot;` → JSON-like string.
   - Zatím nepracuje s `llm/config.yaml` a nemá defaultní parametry podle use_case.

2. `llm/config.py`
   - Načte `_CONFIG = load_yaml(&quot;llm/config.yaml&quot;)`.
   - Definuje vlastní `LLMMessage` (role: str, content: str).
   - `LLMClient.__init__`:
     - vyžaduje `OPENAI_API_KEY` (jinak `RuntimeError` – žádný fallback).
     - vytváří `OpenAI(api_key=api_key)`.
   - `_get_params(use_case)`:
     - `if use_case == &quot;legal_analysis&quot;` → použije:
       - `model_defaults.legal_analysis_model`,
       - `temperature.legal_analysis`,
       - `max_tokens.legal_analysis`.
     - jinak → `helper_model` + `helper` temperature + `helper` max_tokens.
   - `chat(use_case, messages)`:
     - nepodporuje přepis `temperature`/`max_tokens`,
     - přímo volá OpenAI a vrací `resp.choices[0].message.content.strip()`.

3. `llm/config.yaml`
   - Definuje:
     - `provider: &quot;openai&quot;`,
     - `model_defaults.legal_analysis_model`, `helper_model`,
     - `temperature.legal_analysis/helper`,
     - `max_tokens.legal_analysis/helper`.

4. `llm/prompts/`
   - V tomto výpisu nebyl konkrétní obsah načten, ale typicky obsahuje prompt šablony pro enginy.

### 2.2 Hlavní problémy

- Dva různé LLM klienty:
  - rozdílné API (signatura `chat`),
  - rozdílný mechanismus konfigurace (YAML vs env),
  - rozdílný přístup k absenci `OPENAI_API_KEY` (fatal vs fallback na mock).
- YAML konfigurace modelů se nepromítá do „nového“ klienta.
- Use-case řízení parametrů je jen v `llm/config.py`.
- Není jasně deklarováno, který klient je „oficiální“.

### 2.3 Směr řešení (v souladu s požadavky)

- „Oficiální“ klient: **`llm/client.py`**.
- `llm/config.py`:
  - označit jako **legacy/deprecated**,
  - postupně odstavit; nové enginy a runtime mají používat `llm.client.LLMClient`.
- Přenést čtení `llm/config.yaml` a mapování `use_case → (model, temperature, max_tokens)` do **`llm/client.py`**:
  - přes `runtime.config_loader.load_yaml` (stejný mechanismus jako doteď),
  - s možností přepsat `temperature`/`max_tokens` argumentem `chat(...)`.
- Zachovat robustní fallback:
  - pokud není `OPENAI_API_KEY` nebo import selže → vždy mock backend, žádný runtime crash.
- Sjednotit enginy:
  - `core_legal`, `judikatura`, `intent`, `risk`, `output` mají volat výhradně nový `LLMClient.chat(...)`,
  - žádné přímé sahání do `llm/config.py` nebo vlastní OpenAI klienty.

---

## 3. Návrh konkrétních kroků

### Krok 1: Zvolit „oficiální“ klient

1. V dokumentaci (README nebo samostatný `docs/llm.md`) a v kódu:
   - deklarovat `llm/client.py` jako **jediný oficiální LLM klient**.
   - `llm/config.py` označit na úrovni komentářů jako deprecated.

---

### Krok 2: Rozšířit `llm/client.py` o čtení `llm/config.yaml`

Navrhovaný koncept (bez zápisu, jen struktura):

1. Na začátek `llm/client.py` doplnit import a lazily načtenou konfiguraci:

```python
from runtime.config_loader import load_yaml
from typing import Dict, Tuple

_LLM_CONFIG = load_yaml(&quot;llm/config.yaml&quot;)

def _get_default_params(use_case: str) -&gt; Tuple[str, float, int]:
    &quot;&quot;&quot;
    Vrátí (model, temperature, max_tokens) pro daný use_case na základě llm/config.yaml.
    - use_case == &quot;legal_analysis&quot; → legal_analysis_model
    - jinak → helper_model
    &quot;&quot;&quot;
    cfg = _LLM_CONFIG
    if use_case == &quot;legal_analysis&quot;:
        model = cfg[&quot;model_defaults&quot;][&quot;legal_analysis_model&quot;]
        temperature = float(cfg[&quot;temperature&quot;][&quot;legal_analysis&quot;])
        max_tokens = int(cfg[&quot;max_tokens&quot;][&quot;legal_analysis&quot;])
    else:
        model = cfg[&quot;model_defaults&quot;][&quot;helper_model&quot;]
        temperature = float(cfg[&quot;temperature&quot;][&quot;helper&quot;])
        max_tokens = int(cfg[&quot;max_tokens&quot;][&quot;helper&quot;])
    return model, temperature, max_tokens
```

2. Upravovat `LLMClient.chat` tak, aby:
   - kombinoval YAML defaults s volitelnými parametry:

```python
    def chat(
        self,
        use_case: str,
        messages: List[LLMMessage],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -&gt; str:
        # načteme defaulty
        model, default_temp, default_max_tokens = _get_default_params(use_case)

        # umožníme override
        temp = temperature if temperature is not None else default_temp
        max_tok = max_tokens if max_tokens is not None else default_max_tokens

        if self.backend == &quot;openai&quot; and self._openai_client is not None:
            return self._chat_openai(
                model=model,
                messages=messages,
                temperature=temp,
                max_tokens=max_tok,
            )

        return self._chat_mock(use_case, messages)
```

3. Upravit `_chat_openai` signaturu:

```python
    def _chat_openai(
        self,
        model: str,
        messages: List[LLMMessage],
        temperature: Optional[float],
        max_tokens: Optional[int],
    ) -&gt; str:
        try:
            api_messages = [{&quot;role&quot;: m.role, &quot;content&quot;: m.content} for m in messages]

            resp = self._openai_client.chat.completions.create(  # type: ignore[union-attr]
                model=model,
                messages=api_messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            content = getattr(resp.choices[0].message, &quot;content&quot;, None)  # type: ignore[index]
            return content or &quot;&quot;
        except Exception as e:
            return f&quot;[LLM ERROR] {e}&quot;
```

4. Tím pádem:
   - `OPENAI_MODEL` env již není potřeba (model určuje YAML).
   - `use_case` vždy vede k jasnému výběru parametrů.

**Nejasnost k doplnění do reportu:**  
- V `llm/config.yaml` jsou zatím jen dvě větve (`legal_analysis` vs `helper`). Pokud budou další use_case (např. `judikatura_search`, `intent_classification`, `risk_assessment`), bude potřeba rozšířit YAML a `_get_default_params`. To je produktové rozhodnutí – zatím lze všechny ostatní use-cases mapovat na `helper`.

---

### Krok 3: Označit `llm/config.py` jako legacy a deaktivovat přímé použití

1. Do hlavičky `llm/config.py` přidat výrazný komentář:

```python
&quot;&quot;&quot;
DEPRECATED: Tento modul byl nahrazen centralizovaným klientem v llm/client.py.
Nepoužívej ho v novém kódu. Bude odstraněn po migraci všech volání.
&quot;&quot;&quot;
```

2. Pro jistotu (aby se omylem nepoužil):
   - lze ponechat implementaci beze změny (kvůli případným starým importům),
   - nebo, pokud v repozitáři není nikde importován, označit pro budoucí smazání v TODO.

3. Pro Stage 1:
   - jen označit jako legacy,
   - neprovádět „tvrdé“ změny (aby se nepokazila kompatibilita, i když teď reálně zřejmě není používán žádnými enginy).

---

### Krok 4: Zajistit robustitu při chybějícím `OPENAI_API_KEY`

Toto je již v `llm/client.py` do značné míry hotové – v `__init__`:

- pokud `OPENAI_API_KEY` chybí nebo import/klient selže → přepne backend na `&quot;mock&quot;`.

Doporučené drobné vylepšení:

1. Explicitně logovat / poznámkovat důvod fallbacku (např. přes `print` nebo budoucí logger):

```python
        if self.backend == &quot;openai&quot;:
            api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)
            if not api_key:
                # Bez klíče nemá smysl se snažit o openai režim
                # TODO: nahradit loggerem
                # print(&quot;LLMClient: OPENAI_API_KEY not set, falling back to mock backend.&quot;)
                self.backend = &quot;mock&quot;
                return
            ...
```

2. V `_chat_openai` chyba se již převádí na text `&quot;[LLM ERROR] ...&quot;` – to je v souladu s požadavkem „raději degradovat než spadnout“.

---

### Krok 5: Sjednotit volání LLM v enginech

Z poskytnutého výpisu nejsou vidět v enginech žádná přímá volání LLM; nejspíš jsou v souborech, které zatím nemáme načtené (`engines/core_legal/engine.py`, `engines/judikatura/engine.py`, `engines/risk/engine.py`, `engines/output/...`).

**Co je potřeba udělat (plán pro developera):**

1. V každém LLM-based engine:
   - najít všechna místa, kde se:
     - přímo používá `openai` klient,
     - nebo importuje `llm.config.LLMClient`,
     - nebo se nějak jinak sahá na `OPENAI_API_KEY` / `OPENAI_MODEL`.
2. Nahradit za použití `llm.client`:

```python
from llm.client import LLMClient, LLMMessage

llm = LLMClient()

messages = [
    LLMMessage(role=&quot;system&quot;, content=&quot;...prompt...&quot;),
    LLMMessage(role=&quot;user&quot;, content=user_text),
]

response_text = llm.chat(
    use_case=&quot;legal_analysis&quot;,  # nebo jiný use_case dle engine
    messages=messages,
    # optional override:
    # temperature=0.3,
    # max_tokens=1000,
)
```

3. Navržené mapování `use_case`:
   - `engines/core_legal`:
     - hlavní právní analýza → `use_case=&quot;legal_analysis&quot;`.
   - `engines/judikatura`:
     - hledání judikatury → např. `use_case=&quot;jurisprudence_search&quot;` (momentálně v mocku rozlišeno).
   - `engines/intent`:
     - pokud někdy bude LLM verze klasifikace → `use_case=&quot;helper&quot;` / `intent_classification`.
   - `engines/risk`, `engines/output`, `engines/memory`, `engines/safety`:
     - spíš krátké pomocné úlohy → mapovat na helper → `use_case=&quot;helper&quot;` (dokud nebude detailnější config).

4. Zajistit, aby žádný engine již neimportoval `llm.config`:
   - pokud něco takového existuje, přepsat import na `llm.client`.
   - pokud engine bral `model`/`temperature` z jiného YAML (např. `engines/core_legal/config.yaml`), zvážit:
     - zda to nemá být jen „business“ konfigurace (např. režim skeleton/llm),
     - LLM parametry nechat jednotně v `llm/config.yaml`.

---

### Krok 6: Návrh konkrétních diffů (konceptuálně)

#### 6.1 `llm/client.py` – rozšíření o YAML konfiguraci

```diff
@@
-from dataclasses import dataclass
-from typing import List, Optional, Literal, Any
+from dataclasses import dataclass
+from typing import List, Optional, Literal, Any, Tuple
+
+from runtime.config_loader import load_yaml
@@
 Role = Literal[&quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;]
@@
 class LLMMessage:
     role: Role
     content: str
+
+
+_LLM_CONFIG = load_yaml(&quot;llm/config.yaml&quot;)
+
+
+def _get_default_params(use_case: str) -&gt; Tuple[str, float, int]:
+    &quot;&quot;&quot;
+    Vrátí (model, temperature, max_tokens) pro daný use_case
+    na základě llm/config.yaml.
+    &quot;&quot;&quot;
+    cfg = _LLM_CONFIG
+    if use_case == &quot;legal_analysis&quot;:
+        model = cfg[&quot;model_defaults&quot;][&quot;legal_analysis_model&quot;]
+        temperature = float(cfg[&quot;temperature&quot;][&quot;legal_analysis&quot;])
+        max_tokens = int(cfg[&quot;max_tokens&quot;][&quot;legal_analysis&quot;])
+    else:
+        model = cfg[&quot;model_defaults&quot;][&quot;helper_model&quot;]
+        temperature = float(cfg[&quot;temperature&quot;][&quot;helper&quot;])
+        max_tokens = int(cfg[&quot;max_tokens&quot;][&quot;helper&quot;])
+    return model, temperature, max_tokens
@@
     def chat(
         self,
         use_case: str,
         messages: List[LLMMessage],
         temperature: Optional[float] = None,
         max_tokens: Optional[int] = None,
     ) -&gt; str:
@@
-        if self.backend == &quot;openai&quot; and self._openai_client is not None:
-            return self._chat_openai(messages, temperature, max_tokens)
-
-        # fallback / testovací mock
-        return self._chat_mock(use_case, messages)
+        # načtení defaultních parametrů z llm/config.yaml
+        model, default_temp, default_max_tokens = _get_default_params(use_case)
+
+        temp = temperature if temperature is not None else default_temp
+        max_tok = max_tokens if max_tokens is not None else default_max_tokens
+
+        if self.backend == &quot;openai&quot; and self._openai_client is not None:
+            return self._chat_openai(
+                model=model,
+                messages=messages,
+                temperature=temp,
+                max_tokens=max_tok,
+            )
+
+        # fallback / testovací mock
+        return self._chat_mock(use_case, messages)
@@
-    def _chat_openai(
-        self,
-        messages: List[LLMMessage],
-        temperature: Optional[float],
-        max_tokens: Optional[int],
-    ) -&gt; str:
+    def _chat_openai(
+        self,
+        model: str,
+        messages: List[LLMMessage],
+        temperature: Optional[float],
+        max_tokens: Optional[int],
+    ) -&gt; str:
@@
-            model = os.getenv(&quot;OPENAI_MODEL&quot;, &quot;gpt-4.1-mini&quot;)
-
             # převod do dict formátu, kterou openai klient očekává
             api_messages = [
                 {&quot;role&quot;: m.role, &quot;content&quot;: m.content} for m in messages
             ]
@@
-                model=model,
+                model=model,
                 messages=api_messages,
                 temperature=temperature,
                 max_tokens=max_tokens,
             )
```

#### 6.2 `llm/config.py` – označení jako deprecated

```diff
-# llm/client.py
-
-from __future__ import annotations
+&quot;&quot;&quot;
+DEPRECATED:
+  Tento modul byl nahrazen centralizovaným klientem v llm/client.py.
+  Nepoužívej ho v novém kódu. Bude odstraněn po migraci všech volání.
+&quot;&quot;&quot;
+
+from __future__ import annotations
```

(Není nutné dělat další změny, pokud žádný kód tento modul nepoužívá.)

---

### Krok 7: Dopad na testy a kompatibilitu

- Ze seznamu testů:
  - `tests/test_domain_rules.py`
  - `tests/test_judikatura_engine.py`
  - `tests/test_runtime_pipeline.py`
  - `tests/test_core_legal_engine.py`
- Dosavadní `LLMClient` defaultuje na backend `&quot;mock&quot;` (není-li `LLM_BACKEND=openai` + klíč).
  - To by mělo zachovat stávající chování testů, pokud:
    - testy nebyly závislé na předchozím `llm/config.py` (což z dostupného výpisu nevypadá),
    - testy nečekají konkrétní hodnotu `OPENAI_MODEL` (kterou nyní nahradí model z YAML, ale v mock režimu to stejně není relevantní).
- Změna signatury `_chat_openai` je vnitřní; `chat(...)` API zůstává stejné.
- Pokud by existoval test, který:
  - explicitně importuje `llm.config.LLMClient` nebo `llm.config.LLMMessage`,
  - nebo zkoumá `OPENAI_MODEL`,
  → bylo by nutné:
    - buď test upravit na nový klient (`llm.client`),
    - nebo dočasně poskytnout kompatibilní wrapper (ale to by odporovalo snaze odstranit duplicitu).
- Doporučení:
  - Spustit testy po úpravách.
  - Pokud něco selže, zapsat konkrétní důvod do reportu (např. „test X přímo importuje llm.config“), a upravit test tak, aby:
    - používal `llm.client.LLMClient`,
    - nevyžadoval přítomnost reálného API key (testy by měly běžet v mock režimu).

---

## 4. Nejasnosti / otevřené otázky

1. **Skutečné použití `llm/config.py`**:
   - Z daného výpisu není vidět, že by jej někdo importoval.
   - Je potřeba projít celý repo (grep na `from llm.config import` / `llm.config.LLMClient`) a potvrdit, že není používán.
2. **Granularita use_case v YAML**:
   - Dnes jsou jen `legal_analysis` vs `helper`.
   - Pro budoucí čistotu by bylo vhodné:
     - buď definovat další klíče (`judikatura_search`, `risk_assessment`, `output_polish`, …),
     - nebo formálně říct, že všechny tyto use-case patří do kategorie „helper“.
3. **Potenciální konfigurace LLM parametrů v engine-specific YAML**:
   - Např. `engines/core_legal/config.yaml` může obsahovat něco jako „mode: skeleton/llm, llm_use: legal_analysis“.
   - Je potřeba zkontrolovat, zda se tam nesnaží duplikovat LLM parametry; pokud ano, navrhnout jejich odstranění ve prospěch centralizace v `llm/config.yaml`.

---

## 5. Shrnutí navržených kroků pro vývojáře

1. **Oficiální klient**
   - Potvrdit `llm/client.py` jako jediného LLM klienta.
   - Přidat dokumentační zmínku (README/docs).

2. **Konfigurace z YAML**
   - Implementovat v `llm/client.py` funkci `_get_default_params(use_case)` využívající `llm/config.yaml`.
   - Upravit `LLMClient.chat` a `_chat_openai` podle navrženého diffu.

3. **Deprecace starého klienta**
   - Označit `llm/config.py` jako deprecated (komentář).
   - Projít repo a ověřit, že nikdo tento modul nepoužívá; pokud ano, přepsat importy na `llm.client`.

4. **Sjednocení engine volání**
   - Otevřít:
     - `engines/core_legal/engine.py`
     - `engines/judikatura/engine.py`
     - `engines/risk/engine.py`
     - `engines/output/*`
   - Najít všechna LLM volání a přepsat je na `LLMClient.chat(use_case, messages, ...)` s použitím `LLMMessage`.
   - Zajistit, aby žádný engine nepoužíval přímo `openai` ani `llm/config.py`.

5. **Testy**
   - Spustit stávající testy.
   - Pokud některý test očekává staré chování (import z `llm.config` nebo závislost na reálném API), upravit jej:
     - tak, aby pracoval v mock režimu,
     - s oficiálním klientem `llm.client`.

6. **Další fáze (Stage 2+) – návrh do budoucna**
   - Rozšířit `llm/config.yaml` o explicitní sekci `use_cases:` s parametry pro jednotlivé enginy (místo dvou pevných větví).
   - Přidat jednoduchý logger / structured logging do `LLMClient` pro lepší debugging (místo případných `print`).
   - Připravit jednotný enum/konstanty pro use_case (např. v `llm/client.py`), aby se minimalizovaly překlepy v názvech.
================================================================================
</pre>
</body>
</html>
